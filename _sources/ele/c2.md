# C2-Overview of Supervised Learning
## Variable Types and Terminology
In this section, the authors introduce a number of variables that are frequently used in machine learning fields. The definition is very clear and is very important for beginers. 

## Two Simple Approaches to Prediction: Least Squares and Nearest Neighbors
**Least Squares**
Formula: $$
\hat{Y}=\hat{\beta}_{0}+\sum_{j=1}^{p}X_{j}\hat{\beta}_{j}=X^T{\beta}. 
$$
Fun fact: $(X,\hat{Y})$ represents a hyperplane, which means if $(X,Y)$ is not close to a hyperplane, then the prediction will be very inaccurate.

Assumption:
Pros:
Cons:
When to use:
How to compute the parameter $\beta$: 
1. We can build a loss function with least squares, i.e., $$RSS(\beta)=\sum_{i=1}^N(y_i-x_i^T\beta)^2=(y-\bold{X}\beta)^T(y-\bold{Y}\beta),$$ where $\bold{X}$ is a $N\times p$ matrix with each row as an input vector and $\bold{y}$ is a $N$-vector.
2. We try to find a good $\beta$ to minimize the loss function $RSS(\beta)$. Since this is a quadratic function, we know the minimum value can be get when the derivative of this function w.r.t $\beta$ is zero, i.e., $$
{\bf X}^{T}({\bf y}-{\bf X}\beta)=0. 
$$
if $X^TX$ is nonsingular, then we have the close-form solution 
$$
\beta=({\bf X}^{T}{\bf X})^{-1}{\bf X}^{T}{\bf y}, 
$$

**K Nearest Neighbors**
Assumption:
Pros:
Cons:
When to use: